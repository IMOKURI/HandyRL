
env_args:
    # env: 'TicTacToe'
    # source: 'handyrl.envs.tictactoe'
    # env: 'Geister'
    # source: 'handyrl.envs.geister'
    env: 'HungryGeese'
    source: 'handyrl.envs.kaggle.hungry_geese'


train_args:
    turn_based_training: False
    observation: False
    gamma: 0.8
    forward_steps: 32
    compress_steps: 4
    entropy_regularization: 2.0e-3
    entropy_regularization_decay: 0.3
    update_episodes: 500  # このゲーム数ごとにモデルを更新する
    batch_size: 500  # GPU memory 12GB
    minimum_episodes: 10000  # 最初にこのゲーム数行ってから損失計算が始まる
    maximum_episodes: 500000  # メモリ上に保持する最大ゲーム数 CPU memory 64GB
    num_batchers: 7
    eval_rate: 0.1
    worker:
        num_parallel: 8
    lambda: 0.7
    # https://qiita.com/YuriCat/items/5c2a676a67981ecca94a
    # 最初は TD そのあと VTRACE
    policy_target: 'TD' # 'UPGO' 'VTRACE' 'TD' 'MC'
    value_target: 'TD' # 'UPGO' 'VTRACE' 'TD' 'MC'
    seed: 22
    restart_epoch: 0  # このモデル番号から学習を再開する


worker_args:
    server_address: '127.0.0.1'
    num_parallel: 8

